{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connect 4 board class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython import display\n",
    "\n",
    "'''\n",
    "Class for the connect 4 game:\n",
    "Number of rows = 4, Number of columns = 5\n",
    "The board will be a 2D Numpy array consisting of 0s, 1s, and 2s (where 1 is player 1, 2 is player 2, 0 is an empty slot)\n",
    "Rewards are as follows: {win: 1, draw: -0.5, lose: -1} (we want to maximize winning)\n",
    "'''\n",
    "\n",
    "class C4:\n",
    "    def __init__(self):\n",
    "        self.width = 7\n",
    "        self.height = 6\n",
    "        self.state = np.zeros([self.height, self.width], dtype=np.uint8)\n",
    "        self.players = {'P1': 1, 'P2': 2}\n",
    "        self.rewards = {'Win': 1, 'Draw': -0.5, 'Lose': -1}\n",
    "        self.Finished = False\n",
    "        self.actions = [0, 1, 2, 3, 4, 5, 6]\n",
    "        \n",
    "    def resetGame(self):\n",
    "        self.__init__()\n",
    "\n",
    "    \n",
    "    '''\n",
    "    Function for returning the columns which are not full (the topmost slot in the column should be a 0)\n",
    "    '''\n",
    "\n",
    "    def free_cols(self):\n",
    "        return [col for col in range(self.width) if self.state[0, col] == 0]\n",
    "\n",
    "\n",
    "\n",
    "    '''\n",
    "    Function for checking winning conditions\n",
    "    Input will be the player, row & col of move played\n",
    "    Search for win in the col, row and the two diagonals\n",
    "    '''\n",
    "    \n",
    "    def check_vertical(self, sub_str, col):\n",
    "        return sub_str in ' '.join(map(str, self.state[:, col]))\n",
    "\n",
    "    \n",
    "    def check_horizontal(self, sub_str, row):\n",
    "        return sub_str in ' '.join(map(str, self.state[row, :]))\n",
    "    \n",
    "    def check_diagonal(self, sub_str, row, col):\n",
    "        left_diagonal = ''\n",
    "\n",
    "        #first go to the lefmost point in the left diagonal of the row, col\n",
    "        i = row - min(row, col)\n",
    "        j = col - min(row, col)\n",
    "        while i < self.height and j < self.width:\n",
    "            left_diagonal += f'{self.state[i, j]} '\n",
    "            i+=1\n",
    "            j+=1\n",
    "        \n",
    "        right_diagonal = ''\n",
    "\n",
    "        #first go to the rightmost point in the right diagonal of the row, col\n",
    "        i  = row - min(row, 6 - col)\n",
    "        j = col + min(row, 6 - col)\n",
    "        while i < self.height and j > 0:\n",
    "            right_diagonal += f'{self.state[i, j]} '\n",
    "            i+=1\n",
    "            j-=1\n",
    "\n",
    "        return sub_str in left_diagonal or sub_str in right_diagonal\n",
    "    \n",
    "    #we just need to check if the board is full \n",
    "    def is_draw(self):\n",
    "        for col in range(self.width):\n",
    "            if self.state[0][col] == 0:\n",
    "                return False\n",
    "        self.Finished = True\n",
    "        return True\n",
    "    \n",
    "    def render(self):\n",
    "        rendered_board_state = self.state.copy().astype(str)\n",
    "        rendered_board_state[self.state == 0] = ' '\n",
    "        rendered_board_state[self.state == 1] = 'O'\n",
    "        rendered_board_state[self.state == 2] = 'X'\n",
    "        display(pd.DataFrame(rendered_board_state))\n",
    "\n",
    "    def check_win(self, player, row, col):\n",
    "        win_substr = ' '.join([str(self.players[player])] * 4)\n",
    "        #if either of the conditions passes, the current player has won\n",
    "        if self.check_vertical(win_substr, col) or self.check_horizontal(win_substr, row) or self.check_diagonal(win_substr, row, col):\n",
    "            self.Finished = True\n",
    "        \n",
    "        if self.Finished:\n",
    "            return self.rewards['Win']\n",
    "        elif self.is_draw():\n",
    "            return self.rewards['Draw']\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    '''\n",
    "    Function for making a move.\n",
    "    If the move is valid, drop the token at the lowest empty space in the column\n",
    "    Once the move is made, check winning conditions\n",
    "\n",
    "    '''\n",
    "\n",
    "    def move(self, player, col):\n",
    "        #check if there is free space in the column\n",
    "        if self.state[0, col] == 0:\n",
    "            row = np.where(self.state[:, col] == 0)[0][-1]\n",
    "            self.state[row, col] = self.players[player]\n",
    "\n",
    "\n",
    "        else:\n",
    "            print('Invalid move')\n",
    "\n",
    "        return self.state.copy(), self.check_win(player, row, col)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experience Replay Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Experience Replay will be used to train.\n",
    "Any transition that is observed will be stored: (state, action taken, reward received, next state)\n",
    "We can randomly sample from this list to use for training instead of training on each state-action pair\n",
    "'''\n",
    "\n",
    "import random\n",
    "\n",
    "class Expr_Replay:\n",
    "    def __init__(self):\n",
    "        self.store = []\n",
    "\n",
    "    def sample(self, num):\n",
    "        return random.sample(self.store, num)\n",
    "    \n",
    "    def add(self, transition):\n",
    "        self.store.append(transition)\n",
    "\n",
    "    def _len(self):\n",
    "        return len(self.store)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network to approximate the Q table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DQN, self).__init__()\n",
    "\n",
    "        #convolutional layers\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=5, padding=2)\n",
    "        self.conv2 = nn.Conv2d(32, 32, kernel_size=5, padding=2)\n",
    "\n",
    "        #fully connected layers\n",
    "        self.fc1 = nn.Linear(32 * 6 * 7, 42)\n",
    "        self.fc2 = nn.Linear(42, 42)\n",
    "        self.out = nn.Linear(42, 7)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters used in training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self):\n",
    "        self.learning_rate = 0.001\n",
    "        self.batch_size = 200\n",
    "        self.gamma = 0.9\n",
    "        self.e_max = 1.0\n",
    "        self.e_min = 0.01\n",
    "        self.decay_rate = 0.001\n",
    "        self.env = C4()\n",
    "        self.memory = Expr_Replay()\n",
    "        self.main_net = DQN().to(device)\n",
    "        self.target_net = DQN().to(device)\n",
    "        self.target_net.load_state_dict(self.main_net.state_dict())\n",
    "        self.optimizer = optim.Adam(self.main_net.parameters(), lr=self.learning_rate)\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "        self.steps_done = 0\n",
    "        self.episodes = 25000\n",
    "        self.target_lag = 15\n",
    "        self.training_history = []\n",
    "\n",
    "\n",
    "\n",
    "    #exponential decay used for epsilon\n",
    "    def epsilon_decay(self, step):\n",
    "        return self.e_min + (self.e_max - self.e_min) * math.exp(-self.decay_rate * step)\n",
    "\n",
    "\n",
    "    #add batch size and channel to make it ready for a conv layer\n",
    "    def transform_input(self, state):\n",
    "        return torch.tensor(state, dtype=torch.float, device=device).view(1, 1, *state.shape)\n",
    "\n",
    "    '''\n",
    "    Function for carrying out epsilon-greedy strategy for the agent\n",
    "    '''\n",
    "    def epsilon_greedy(self, state, free_actions, step=None, training=True):\n",
    "        state = self.transform_input(state)\n",
    "\n",
    "        if training:\n",
    "            threshold = self.epsilon_decay(step)\n",
    "        else:\n",
    "            threshold = 0\n",
    "            \n",
    "        #if less then epsilon, then choose a random available action\n",
    "        #if greater than epsilon, then use the main NN to exploit\n",
    "        if random.random() < threshold:\n",
    "            return random.choice(free_actions)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                actions = self.main_net(state)[0, :]\n",
    "                vals = [actions[i].cpu().numpy() for i in free_actions]\n",
    "                return free_actions[np.argmax(vals)]\n",
    "\n",
    "    def rand_agent(self, free_cols):\n",
    "        return random.choice(free_cols)\n",
    "\n",
    "\n",
    "    def optimize(self):\n",
    "        transitions = self.memory.sample(self.batch_size)\n",
    "        state_batch, action_batch, reward_batch, next_state_batch = zip(*[(np.expand_dims(m[0], axis=0), [m[1]], m[2], np.expand_dims(m[3], axis=0)) for m in transitions])\n",
    "\n",
    "        state_batch = torch.tensor(state_batch, dtype=torch.float, device=device)\n",
    "        action_batch = torch.tensor(action_batch, dtype=torch.long, device=device)\n",
    "        reward_batch = torch.tensor(reward_batch, dtype=torch.float, device=device)\n",
    "\n",
    "        # for assigning terminal state value = 0 later\n",
    "        non_final_mask = torch.tensor(tuple(map(lambda s_: s_[0] is not None, next_state_batch)), device=device)\n",
    "        non_final_next_state = torch.cat([torch.tensor(s_, dtype=torch.float, device=device).unsqueeze(0) for s_ in next_state_batch if s_[0] is not None])\n",
    "\n",
    "        # prediction from policy_net\n",
    "        state_action_values = self.main_net(state_batch).gather(1, action_batch)\n",
    "        \n",
    "        # truth from target_net, initialize with zeros since terminal state value = 0\n",
    "        next_state_values = torch.zeros(self.batch_size, device=device)\n",
    "        # tensor.detach() creates a tensor that shares storage with tensor that does not require grad\n",
    "        next_state_values[non_final_mask] = self.target_net(non_final_next_state).max(1)[0].detach()\n",
    "        # compute the expected Q values\n",
    "        expected_state_action_values = (next_state_values * self.gamma) + reward_batch\n",
    "\n",
    "        loss = self.loss_fn(state_action_values, expected_state_action_values.unsqueeze(1)) # torch.tensor.unsqueeze returns a copy\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    ''' \n",
    "    Function for training our main NN. For each episode, we will randomize who makes the first move so that our agent has experience with both situations.\n",
    "    Need to rewrite a lot\n",
    "    '''\n",
    "    def train(self):\n",
    "        for i in range(self.episodes):\n",
    "            self.env.resetGame()\n",
    "\n",
    "            #select random player to go first: P1 is always our agent and P2 is always a random agent\n",
    "            #exposes our agent to episodes where it starts with the second turn\n",
    "            first = random.choice(['P1', 'P2'])\n",
    "            if first == 'P2':\n",
    "                free_actions = self.env.free_cols()\n",
    "                a_p2 = self.rand_agent(free_actions)\n",
    "                s_p2_curr, reward_p2 = self.env.move('P2', a_p2)\n",
    "                s_p1 = s_p2_curr\n",
    "            else:\n",
    "                s_p1 = self.env.state.copy()\n",
    "\n",
    "\n",
    "            #main loop for each episode\n",
    "            while True:\n",
    "                free_actions = self.env.free_cols()\n",
    "                a_p1 = self.epsilon_greedy(s_p1, free_actions, self.steps_done)\n",
    "                s_p1_curr, reward_p1 = self.env.move('P1', a_p1)\n",
    "                self.steps_done += 1\n",
    "\n",
    "                if self.env.Finished:\n",
    "                    self.memory.add([s_p1, a_p1, reward_p1, None])\n",
    "                    break\n",
    "\n",
    "                free_actions = self.env.free_cols()\n",
    "                a_p2 = self.rand_agent(free_actions)\n",
    "                s_p2_curr, reward_p2 = self.env.move(\"P2\", a_p2)\n",
    "\n",
    "                if self.env.Finished:\n",
    "                    if reward_p2 == 1:\n",
    "                        self.memory.add([s_p1, a_p1, -1, None])\n",
    "                    else:\n",
    "                        self.memory.add([s_p1, a_p1, -0.5, None])\n",
    "                    break\n",
    "                \n",
    "                self.memory.add([s_p1, a_p1, -0.1, s_p2_curr])\n",
    "                s_p1 = s_p2_curr\n",
    "\n",
    "                #optimize the main net\n",
    "                if self.memory._len() >= self.batch_size:\n",
    "                    self.optimize()\n",
    "            \n",
    "            if i % self.target_lag == self.target_lag - 1:\n",
    "                self.target_net.load_state_dict(self.main_net.state_dict())\n",
    "\n",
    "        print('Completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent()\n",
    "agent.train()\n",
    "torch.save(agent.main_net.state_dict(), 'C4.pth')\n",
    "from google.colab import files\n",
    "files.download('C4.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Simulate a game against the trained bot\n",
    "\n",
    "env = C4()\n",
    "model = torch.load('C4.pth')\n",
    "p1_turn = True\n",
    "last = 'P1'\n",
    "env.render()\n",
    "while not env.Finished:\n",
    "    if p1_turn:\n",
    "        last = 'P1'\n",
    "        while col not in env.free_cols():\n",
    "            col = input(\"Enter a column number: \")\n",
    "        env.move('P1', col)\n",
    "        env.render()    \n",
    "        p1_turn = not p1_turn\n",
    "\n",
    "    else:\n",
    "        last = 'P2'\n",
    "        available = env.free_cols()\n",
    "        actions = model(env.state.copy())[0, :]\n",
    "        vals = [actions[i] for i in available]\n",
    "        move = available[np.argmax(vals)]\n",
    "        env.move('P2', move)\n",
    "        env.render()\n",
    "        p1_turn = not p1_turn\n",
    "if last == 'P1':\n",
    "    print('You have won against the bot!')\n",
    "else:\n",
    "    print('The bot has won against you!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
