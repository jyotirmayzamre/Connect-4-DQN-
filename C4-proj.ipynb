{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connect 4 board class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "'''\n",
    "Class for the connect 4 game:\n",
    "Number of rows = 4, Number of columns = 5\n",
    "The board will be a 2D Numpy array consisting of 0s, 1s, and 2s (where 1 is player 1, 2 is player 2, 0 is an empty slot)\n",
    "Rewards are as follows: {win: 1, draw: -0.5, lose: -1} (we want to maximize winning)\n",
    "'''\n",
    "\n",
    "class C4:\n",
    "    def __init__(self):\n",
    "        self.width = 7\n",
    "        self.height = 6\n",
    "        self.state = np.zeros([self.height, self.width], dtype=np.uint8)\n",
    "        self.players = {'P1': 1, 'P2': 2}\n",
    "        self.rewards = {'Win': 1, 'Draw': -0.5, 'Lose': -1}\n",
    "        self.Finished = False\n",
    "        self.actions = [0, 1, 2, 3, 4, 5, 6]\n",
    "        \n",
    "    def resetGame(self):\n",
    "        self.__init__()\n",
    "\n",
    "    \n",
    "    '''\n",
    "    Function for returning the columns which are not full (the topmost slot in the column should be a 0)\n",
    "    '''\n",
    "\n",
    "    def free_cols(self):\n",
    "        return [col for col in range(self.width) if self.state[0, col] == 0]\n",
    "\n",
    "\n",
    "\n",
    "    '''\n",
    "    Function for checking winning conditions\n",
    "    Input will be the player, row & col of move played\n",
    "    Search for win in the col, row and the two diagonals\n",
    "    '''\n",
    "    \n",
    "    def check_vertical(self, sub_str, col):\n",
    "        return sub_str in self.state[:, col].astype(str)\n",
    "\n",
    "    \n",
    "    def check_horizontal(self, sub_str, row):\n",
    "        return sub_str in self.state[row, :].astype(str)\n",
    "    \n",
    "    def check_diagonal(self, sub_str, row, col):\n",
    "        left_diagonal = ''\n",
    "\n",
    "        #first go to the lefmost point in the left diagonal of the row, col\n",
    "        i = row - min(row, col)\n",
    "        j = col - min(row, col)\n",
    "        while i < self.height and j < self.width:\n",
    "            left_diagonal += f'{self.state[row, col]} '\n",
    "            i+=1\n",
    "            j+=1\n",
    "        \n",
    "        right_diagonal = ''\n",
    "\n",
    "        #first go to the rightmost point in the right diagonal of the row, col\n",
    "        i  = row - min(row, col)\n",
    "        j = col + min(row, col)\n",
    "        while i < self.height and j > 0:\n",
    "            right_diagonal += f'{self.state[row, col]} '\n",
    "            i+=1\n",
    "            j-=1\n",
    "\n",
    "        return sub_str in left_diagonal or sub_str in right_diagonal\n",
    "    \n",
    "    #we just need to check if the board is full \n",
    "    def is_draw(self):\n",
    "        for col in range(self.width):\n",
    "            if self.state[0][col] == 0:\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    def check_win(self, player, row, col):\n",
    "        win_substr = ' '.join([self.players[player]] * 4)\n",
    "        #if either of the conditions passes, the current player has won\n",
    "        if self.check_vertical(win_substr, col) or self.check_horizontal(win_substr, row) or self.check_diagonal(win_substr, row, col):\n",
    "            self.Finished = True\n",
    "        \n",
    "        if self.Finished:\n",
    "            return self.rewards['Win']\n",
    "        elif self.is_draw():\n",
    "            return self.rewards['Draw']\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    '''\n",
    "    Function for making a move.\n",
    "    If the move is valid, drop the token at the lowest empty space in the column\n",
    "    Once the move is made, check winning conditions\n",
    "\n",
    "    '''\n",
    "\n",
    "    def move(self, player, col):\n",
    "        #check if there is free space in the column\n",
    "        if self.state[0, col] == 0:\n",
    "            row = np.where(self.state[:, col])[0][-1]\n",
    "            self.state[row, col] = self.players[player]\n",
    "            return self.state.copy(), self.check_win(player, row, col)\n",
    "\n",
    "\n",
    "        else:\n",
    "            print('Invalid move')\n",
    "            return self.state.copy(), 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experience Replay Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Experience Replay will be used to train.\n",
    "Any transition that is observed will be stored: (state, action taken, reward received, next state)\n",
    "We can randomly sample from this list to use for training instead of training on each state-action pair\n",
    "'''\n",
    "\n",
    "import random\n",
    "\n",
    "class Expr_Replay:\n",
    "    def __init__(self):\n",
    "        self.store = []\n",
    "\n",
    "    def sample(self, num):\n",
    "        return random.sample(self.store, num)\n",
    "    \n",
    "    def add(self, transition):\n",
    "        self.store.append(transition)\n",
    "\n",
    "    def _len(self):\n",
    "        return len(self.store)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network to approximate the Q table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DQN, self).__init__()\n",
    "\n",
    "        #convolutional layers\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=5, padding=2)\n",
    "        self.conv2 = nn.Conv2d(1, 32, kernel_size=5, padding=2)\n",
    "\n",
    "        #fully connected layers\n",
    "        self.fc1 = nn.Linear(32 * 6 * 7, 42)\n",
    "        self.fc2 = nn.Linear(42, 42)\n",
    "        self.out = nn.Linear(42, 7)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters used in training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import math\n",
    "\n",
    "learning_rate = 0.001\n",
    "batch_size = 200\n",
    "gamma = 0.9\n",
    "e_max = 1.0\n",
    "e_min = 0.01\n",
    "decay_rate = 0.001\n",
    "env = C4()\n",
    "memory = Expr_Replay()\n",
    "main_net = DQN()\n",
    "target_net = DQN()\n",
    "target_net.load_state_dict(main_net.state_dict())\n",
    "optimizer = optim.Adam(main_net.parameters(), lr=learning_rate)\n",
    "loss_fn = nn.MSELoss()\n",
    "steps_done = 0\n",
    "episodes = 25000\n",
    "target_lag = 15 #update the target net every 15 episodes\n",
    "\n",
    "\n",
    "#exponential decay used for epsilon\n",
    "def epsilon_decay(step):\n",
    "    return e_min + (e_max - e_min) * math.exp(-decay_rate * step)\n",
    "\n",
    "\n",
    "#add batch size and channel to make it ready for a conv layer\n",
    "def transform_input(state):\n",
    "    return torch.tensor(state, dtype=torch.float).view(1, 1, *state.shape)\n",
    "\n",
    "'''\n",
    "Function for carrying out epsilon-greedy strategy for the agent\n",
    "'''\n",
    "def epsilon_greedy(state, free_actions, step):\n",
    "    state = transform_input(state)\n",
    "\n",
    "    threshold = epsilon_decay(step)\n",
    "\n",
    "    #if less then epsilon, then choose a random available action\n",
    "    #if greater than epsilon, then use the main NN to exploit\n",
    "    if random.random() < threshold:\n",
    "        return random.choice(free_actions)\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            actions = main_net(state)[0, :]\n",
    "            vals = [actions[i] for i in free_actions]\n",
    "            return free_actions[np.argmax(vals)]\n",
    "\n",
    "def rand_agent(free_cols):\n",
    "    return random.choice(free_cols)\n",
    "\n",
    "\n",
    "def optimize():\n",
    "    pass\n",
    "\n",
    "''' \n",
    "Function for training our main NN. For each episode, we will randomize who makes the first move so that our agent has experience with both situations.\n",
    "\n",
    "'''\n",
    "def train():\n",
    "    for i in range(episodes):\n",
    "        env.resetGame()\n",
    "\n",
    "        #select random player to go first: P1 is always our agent and P2 is always a random agent\n",
    "        #exposes our agent to episodes where it starts with the second turn\n",
    "        first = random.choice(env.players)\n",
    "        if first == 'P2':\n",
    "            free_actions = env.free_cols()\n",
    "            a_p2 = rand_agent(free_actions)\n",
    "            s_p2_curr, reward_p2 = env.move('P2', a_p2)\n",
    "            s_p1 = s_p2_curr\n",
    "        else:\n",
    "            s_p1 = env.state.copy()\n",
    "\n",
    "        #main loop for each episode\n",
    "        while True:\n",
    "            free_actions = env.free_cols()\n",
    "            a_p1 = epsilon_greedy(s_p1, free_actions, steps_done)\n",
    "            s_p1_curr, reward_p1 = env.move('P1', a_p1)\n",
    "\n",
    "            if env.Finished:\n",
    "                if reward_p1 == 1:\n",
    "                    memory.add([s_p1, a_p1, 1, None])\n",
    "                else:\n",
    "                    memory.add([s_p1, a_p1, -0.5, None])\n",
    "                break\n",
    "\n",
    "            free_actions = env.free_cols()\n",
    "            a_p2 = rand_agent(free_actions)\n",
    "            s_p2_curr, reward_p2 = env.move(\"P2\", a_p2)\n",
    "\n",
    "            if env.Finished:\n",
    "                if reward_p2 == 1:\n",
    "                    memory.add([s_p1, a_p1, -1, None])\n",
    "                else:\n",
    "                    memory.add([s_p1, a_p1, -0.5, None])\n",
    "                break\n",
    "\n",
    "            s_p1 = s_p2_curr\n",
    "\n",
    "            #optimize the main net\n",
    "            optimize()\n",
    "        \n",
    "        if i % target_lag == target_lag - 1:\n",
    "            target_net.load_state_dict(main_net.state_dict())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
