{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connect 4 board class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "'''\n",
    "Class for the connect 4 game:\n",
    "Number of rows = 4, Number of columns = 5\n",
    "The board will be a 2D Numpy array consisting of 0s, 1s, and 2s (where 1 is player 1, 2 is player 2, 0 is an empty slot)\n",
    "Rewards are as follows: {win: 1, draw: -0.5, lose: -1} (we want to maximize winning)\n",
    "'''\n",
    "\n",
    "class C4:\n",
    "    def __init__(self):\n",
    "        self.width = 7\n",
    "        self.height = 6\n",
    "        self.state = np.zeros([self.height, self.width], dtype=np.uint8)\n",
    "        self.players = {'P1': 1, 'P2': 2}\n",
    "        self.rewards = {'Win': 1, 'Draw': -0.5, 'Lose': -1}\n",
    "        self.Finished = False\n",
    "        \n",
    "    def resetGame(self):\n",
    "        self.__init__()\n",
    "\n",
    "    \n",
    "    '''\n",
    "    Function for returning the columns which are not full (the topmost slot in the column should be a 0)\n",
    "    '''\n",
    "\n",
    "    def free_cols(self):\n",
    "        return [col for col in range(self.width) if self.state[0, col] == 0]\n",
    "\n",
    "\n",
    "\n",
    "    '''\n",
    "    Function for checking winning conditions\n",
    "    Input will be the player, row & col of move played\n",
    "    Search for win in the col, row and the two diagonals\n",
    "    '''\n",
    "    \n",
    "    def check_vertical(self, sub_str, col):\n",
    "        return sub_str in ' '.join(map(str, self.state[:, col]))\n",
    "\n",
    "    \n",
    "    def check_horizontal(self, sub_str, row):\n",
    "        return sub_str in ' '.join(map(str, self.state[row, :]))\n",
    "    \n",
    "    def check_diagonal(self, sub_str, row, col):\n",
    "        left_diagonal = ''\n",
    "\n",
    "        #first go to the uppermost point in the left diagonal of the row, col\n",
    "        i = row - min(row, col)\n",
    "        j = col - min(row, col)\n",
    "        while i < self.height and j < self.width:\n",
    "            left_diagonal += f'{self.state[i, j]} '\n",
    "            i+=1\n",
    "            j+=1\n",
    "        \n",
    "        right_diagonal = ''\n",
    "\n",
    "        #first go to the uppermost point in the right diagonal of the row, col\n",
    "        i  = row - min(row, 6 - col)\n",
    "        j = col + min(row, 6 - col)\n",
    "        while i < self.height and j >= 0:\n",
    "            right_diagonal += f'{self.state[i, j]} '\n",
    "            i+=1\n",
    "            j-=1\n",
    "\n",
    "        return sub_str in left_diagonal or sub_str in right_diagonal\n",
    "    \n",
    "    #we just need to check if the board is full \n",
    "    def is_draw(self):\n",
    "        for col in range(self.width):\n",
    "            if self.state[0][col] == 0:\n",
    "                return False\n",
    "        self.Finished = True\n",
    "        return True\n",
    "    \n",
    "    #for displaying the board in console\n",
    "    def disp(self):\n",
    "        display(pd.DataFrame(np.where(self.state == 1, 'X', np.where(self.state == 2, 'O', ' '))))\n",
    "\n",
    "    \n",
    "\n",
    "    def check_win(self, player, row, col):\n",
    "        win_substr = ' '.join([str(self.players[player])] * 4)\n",
    "        #if either of the conditions passes, the current player has won\n",
    "        if self.check_vertical(win_substr, col) or self.check_horizontal(win_substr, row) or self.check_diagonal(win_substr, row, col):\n",
    "            self.Finished = True\n",
    "        \n",
    "        if self.Finished:\n",
    "            return self.rewards['Win']\n",
    "        elif self.is_draw():\n",
    "            return self.rewards['Draw']\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    '''\n",
    "    Function for making a move.\n",
    "    If the move is valid, drop the token at the lowest empty space in the column\n",
    "    Once the move is made, check winning conditions\n",
    "\n",
    "    '''\n",
    "\n",
    "    def move(self, player, col):\n",
    "        #check if there is free space in the column\n",
    "        if self.state[0, col] == 0:\n",
    "            row = np.where(self.state[:, col] == 0)[0][-1]\n",
    "            self.state[row, col] = self.players[player]\n",
    "\n",
    "\n",
    "        else:\n",
    "            print('Invalid move')\n",
    "\n",
    "        return self.state.copy(), self.check_win(player, row, col)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experience Replay Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Experience Replay will be used to train.\n",
    "Any transition that is observed will be stored: (state, action taken, reward received, next state)\n",
    "We can randomly sample from this list to use for training instead of training on each state-action pair\n",
    "'''\n",
    "\n",
    "import random\n",
    "\n",
    "class Expr_Replay:\n",
    "    def __init__(self):\n",
    "        self.store = []\n",
    "\n",
    "    def sample(self, num):\n",
    "        return random.sample(self.store, num)\n",
    "    \n",
    "    def add(self, transition):\n",
    "        self.store.append(transition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network to approximate the Q table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "''' \n",
    "The architecure for the DQN is simple and as follows.\n",
    "There are two convolution layers with kernel size 5 and 32 filters. Both are activated used ReLu function.\n",
    "There are 3 fully connected layers where the first two are activated using ReLU and the last is just the output layer.\n",
    "'''\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DQN, self).__init__()\n",
    "\n",
    "        #convolutional layers\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=5, padding=2)\n",
    "        self.convH = nn.Conv2d(32, 32, kernel_size=(1, 4), padding=(0, 3))\n",
    "        self.convV = nn.Conv2d(32, 32, kernel_size=(4, 1), padding=(3, 0))\n",
    "        self.conv2 = nn.Conv2d(32, 32, kernel_size=5, padding=2)\n",
    "\n",
    "        #fully connected layers\n",
    "        self.fc1 = nn.Linear(2880, 42)\n",
    "        self.fc2 = nn.Linear(42, 42)\n",
    "        self.out = nn.Linear(42, 7)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.convH(x))\n",
    "        x = F.relu(self.convV(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "\n",
    "        #flattens the input tensor to (batch, 32 * 6 * 7) for transition\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.out(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters used in training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self):\n",
    "        self.learning_rate = 0.001\n",
    "        self.batch_size = 200\n",
    "        self.gamma = 0.9\n",
    "        self.e_max = 1.0\n",
    "        self.e_min = 0.01\n",
    "        self.decay_rate = 0.001\n",
    "        self.env = C4()\n",
    "        self.memory = Expr_Replay()\n",
    "        self.main_net = DQN().to(device)\n",
    "        self.target_net = DQN().to(device)\n",
    "        self.target_net.load_state_dict(self.main_net.state_dict())\n",
    "        self.optimizer = optim.Adam(self.main_net.parameters(), lr=self.learning_rate)\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "        self.steps_done = 0\n",
    "        self.episodes = 25000\n",
    "        self.target_lag = 15\n",
    "\n",
    "\n",
    "\n",
    "    #exponential decay used for epsilon\n",
    "    def epsilon_decay(self, step):\n",
    "        return self.e_min + (self.e_max - self.e_min) * math.exp(-self.decay_rate * step)\n",
    "\n",
    "\n",
    "    #add batch size and channel to make it ready for a conv layer\n",
    "    def transform_input(self, state):\n",
    "        return torch.tensor(state, dtype=torch.float, device=device).view(1, 1, *state.shape)\n",
    "\n",
    "    '''\n",
    "    Function for carrying out epsilon-greedy strategy for the agent\n",
    "    '''\n",
    "    def epsilon_greedy(self, state, free_actions, step):\n",
    "        state = self.transform_input(state)\n",
    "\n",
    "        threshold = self.epsilon_decay(step)\n",
    "            \n",
    "        #if less then epsilon, then choose a random available action\n",
    "        #if greater than epsilon, then use the main NN to exploit\n",
    "        if random.random() < threshold:\n",
    "            return random.choice(free_actions)\n",
    "        else:\n",
    "            actions = self.main_net(state)[0, :]\n",
    "            vals = [actions[i].detach().cpu().numpy() for i in free_actions]\n",
    "            return free_actions[np.argmax(vals)]\n",
    "\n",
    "    '''\n",
    "    Random Agent that picks a random column\n",
    "    '''\n",
    "    def rand_agent(self, free_cols):\n",
    "        return random.choice(free_cols)\n",
    "\n",
    "\n",
    "    ''' \n",
    "    This is the optimization function for the network\n",
    "    First sample a batch of transitions from the experience replay\n",
    "    Separate the 4 components (states, actions, rewards, next_states) into their own tuples and convert into appropriate tensors\n",
    "    Use the target net to compute max Q values\n",
    "    Predict Q values using the main net\n",
    "    Calculate loss and then update weights\n",
    "    '''\n",
    "    def optimize(self):\n",
    "        samples = self.memory.sample(self.batch_size)\n",
    "        \n",
    "        #split the transitions in their own batches\n",
    "        states, actions, rewards, next_states = zip(*samples)\n",
    "\n",
    "        #transform into tensors\n",
    "        states = self.transform_input(states, dtype=torch.float, device=device)\n",
    "        actions = torch.tensor(actions, dtype=torch.long, device=device)\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float, device=device)\n",
    "\n",
    "        #create a mask for non terminal states\n",
    "        mask = tuple(x is not None for x in next_states)\n",
    "        act_next_states = tuple(x for x in next_states if x is not None)\n",
    "        act_next_states = self.transform_input(act_next_states, dtype=torch.float, device=device)\n",
    "\n",
    "        #predicted Q-values\n",
    "        pred = self.main_net(states).gather(0, actions)\n",
    "\n",
    "        #target Q-values\n",
    "        target = torch.zeros(self.batch_size, device=device)\n",
    "        with torch.no_grad():\n",
    "            target[mask] = self.target_net(act_next_states).max(1)[0]\n",
    "        \n",
    "        target = (target * self.gamma) + rewards\n",
    "\n",
    "        loss = self.loss_fn(pred, target.unsqueeze(1))\n",
    "        \n",
    "        #backpropagation\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "    ''' \n",
    "    Function for training our main NN. For each episode, we will randomize who makes the first move so that our agent has experience with both situations.\n",
    "    Need to rewrite a lot\n",
    "    '''\n",
    "    def train(self):\n",
    "        for i in tqdm(range(self.episodes), desc='Processing'):\n",
    "            self.env.resetGame()\n",
    "            moves = 0\n",
    "\n",
    "            #select random player to go first: P1 is always our agent and P2 is always a random agent\n",
    "            #exposes our agent to episodes where it starts with the second turn\n",
    "            first = random.choice(['P1', 'P2'])\n",
    "            if first == 'P2':\n",
    "                free_actions = self.env.free_cols()\n",
    "                a_p2 = self.rand_agent(free_actions)\n",
    "                s_p2_curr, reward_p2 = self.env.move('P2', a_p2)\n",
    "                s_p1 = s_p2_curr\n",
    "            else:\n",
    "                s_p1 = self.env.state.copy()\n",
    "\n",
    "\n",
    "            #main loop for each episode\n",
    "            while True:\n",
    "                free_actions = self.env.free_cols()\n",
    "                a_p1 = self.epsilon_greedy(s_p1, free_actions, self.steps_done)\n",
    "                s_p1_curr, reward_p1 = self.env.move('P1', a_p1)\n",
    "                self.steps_done += 1\n",
    "                moves += 1\n",
    "\n",
    "                #add an experience to the memory store\n",
    "                if self.env.Finished:\n",
    "                    self.memory.add([s_p1, a_p1, reward_p1, None])\n",
    "                    break\n",
    "\n",
    "                free_actions = self.env.free_cols()\n",
    "                a_p2 = self.rand_agent(free_actions)\n",
    "                s_p2_curr, reward_p2 = self.env.move(\"P2\", a_p2)\n",
    "\n",
    "                #inverse reward is added if the random agent wins\n",
    "                if self.env.Finished:\n",
    "                    if reward_p2 == 1:\n",
    "                        self.memory.add([s_p1, a_p1, -1, None])\n",
    "                    else:\n",
    "                        self.memory.add([s_p1, a_p1, -0.5, None])\n",
    "                    break\n",
    "                \n",
    "                #add a small cost for not winning on this move\n",
    "                #as the number of moves taken increases, increase the cost\n",
    "                self.memory.add([s_p1, a_p1, -0.1 * moves, s_p2_curr])\n",
    "                s_p1 = s_p2_curr\n",
    "\n",
    "                #optimize the main net\n",
    "                if len(self.memory.store) >= self.batch_size:\n",
    "                    self.optimize()\n",
    "            \n",
    "            #update the weights of the target net every 15 episodes\n",
    "            if i % self.target_lag == 0:\n",
    "                self.target_net.load_state_dict(self.main_net.state_dict())\n",
    "\n",
    "        print('Completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent()\n",
    "agent.train()\n",
    "torch.save(agent.main_net.state_dict(), 'C4.pth')\n",
    "from google.colab import files\n",
    "files.download('C4.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  1  2  3  4  5  6\n",
       "0                     \n",
       "1                     \n",
       "2                     \n",
       "3                     \n",
       "4                     \n",
       "5                     "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td></td>\n",
       "      <td>X</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  1  2  3  4  5  6\n",
       "0                     \n",
       "1                     \n",
       "2                     \n",
       "3                     \n",
       "4                     \n",
       "5     X               "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td></td>\n",
       "      <td>X</td>\n",
       "      <td>O</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  1  2  3  4  5  6\n",
       "0                     \n",
       "1                     \n",
       "2                     \n",
       "3                     \n",
       "4                     \n",
       "5     X  O            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Simulate a game against the trained bot\n",
    "import torch\n",
    "\n",
    "env = C4()\n",
    "model = DQN()\n",
    "model.load_state_dict(torch.load('C4.pth'))\n",
    "p1_turn = True\n",
    "last = 'P1'\n",
    "env.disp()\n",
    "while not env.Finished:\n",
    "    if p1_turn:\n",
    "        last = 'P1'\n",
    "        col = int(input(\"Enter a column number: \"))\n",
    "        env.move('P1', col)\n",
    "        env.disp()    \n",
    "        p1_turn = not p1_turn\n",
    "\n",
    "    else:\n",
    "        last = 'P2'\n",
    "        available = env.free_cols()\n",
    "        state = env.state.copy()\n",
    "        state = torch.tensor(state, dtype=torch.float).view(1, 1, *state.shape)\n",
    "        with torch.no_grad():\n",
    "            actions = model(state)[0, :]\n",
    "            vals = [actions[i] for i in available]\n",
    "            move = available[np.argmax(vals)]\n",
    "        \n",
    "        env.move('P2', move)\n",
    "        env.disp()\n",
    "        p1_turn = not p1_turn\n",
    "if last == 'P1':\n",
    "    print('You have won against the bot!')\n",
    "else:\n",
    "    print('The bot has won against you!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
